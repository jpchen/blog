---
layout: post
title: "Thoughts and Reflections on NIPS 2017"
description: "Recapping a long, fruitful year in artificial intelligence"
date: 2017-12-09
tags: [machine-learning, artificial-intelligence, bayesian-inference]
comments: true
share: true
---

This year, the [Conference on Neural Information Processing Systems (NIPS)](https://nips.cc) saw its largest attendance to date, with ___ attendees and over ___ papers submitted. It was held in Long Beach, California. It was also my first NIPS, and so as such, I wanted to document my thoughts and impressions on the conference, the events, and the field of machine learning research in general.

**Disclaimer: The opinions and writings below are completely my own, and do not reflect or represent the positions of any institutions I am affiliated with.**

## Conference

![Picture fountain outside ocnvention center]({{ site.url }}/blog/images/pyro.png "NIPS had over 8000 attendees this year.")

The conference itself could be compared to drinking from a firehose.  In the span of the week, most of the greatest minds in machine learning, cognitive psychology, and neuroscience were all in one place having rich discussions and featuring world class research.  It was incredibly inspiring, yet humbling at the same time.

![PIcture of talk]({{ site.url }}/blog/images/pyro.png "NIPS had over 8000 attendees this year.")

Because of the sheer number of submissions and attendees, most of the interesting events were run massively in parallel, which forced prioritizing and compromise for every talk throughout the day.

![PIcture of uber booth]({{ site.url }}/blog/images/pyro.png "NIPS had over 8000 attendees this year.")
There is an incredible amount of industry interest in machine learning and AI.  Virtually every company threw one or more (lucrative by academic standards) events throughout the week (Intel even brought [Flo Rida to perform](https://twitter.com/hardmaru/status/937223816358412288)!).  Some companies gave interviews and offers on the spot.  Bloomberg [compared the conference to professional sports drafting athletes](https://www.bloomberg.com/news/articles/2017-12-06/demand-for-ai-talent-turns-once-staid-conference-into-draft-day).  I'll leave it to you whether this is a good or bad thing for the field.

![Pyro Meetup]({{ site.url }}/blog/images/pyro.png "Pyro NIPS Meetup")

We hosted a casual meet-up for Pyro contributors and developers.  It was attended by the core Pyro team, Pytorch developers, academic collaborators, and independent researchers. Thanks for coming out!

## Talks

Some highlighted talks given during the conference. This includes tutorials, symposia, keynotes, workshops.

### Reverse Engineering Intelligence with Probabilistic Programs

![PIcture of josh giving talk]({{ site.url }}/blog/images/pyro.png "NIPS had over 8000 attendees this year.")

Josh Tenenbaum talked about reverse engineer intuitions and "intelligence" found in toddlers.  Babies are able to perform basic reasoning and inference.  Probabilistic programs attempt to model that cognitive process of modeling intuitive physics and psychology.

The approach is to treat intuitive physics as a pattern recogntion problem.
 In other words, you only need to do a few number of approximate simulations to guess what will happen next.  He showed interesting demonstrations of the models predicting events into the near future (e.g. a stack of blocks falling over).

 Josh also talked about probabilistic programs and its applications in inverse graphics.  *Inverse graphics* is the problem of taking a 2-D or 2.5-D image and derendering it into its components (usually in 3-D).

- jiajun li, tenenbaum, prasheet, "derendering visual graphics"
- applying to agents, intuitive psychology "integrating physics and psychology" (tao gao, yibiao zhao, et al)
program-learning programs (see cog sci artcie)

### Learning from First Principles

Demis Hassabis gave a talk on [Alpha Zero](https://arxiv.org/abs/1712.01815) and learning from first principles.  He started with his usual talk of general intelligence and the accomplishments of Alpha Go.  He then talked about Alpha Zero, which was able to be SOTA systems in Go, Chess, and Shogi after a day of training.  The program was initialized with the same hyperparameters across all three games and zero knowledge of the game other than the rules.

He did note that these RL systems are very data inefficient (for comparison, Alpha Zero played tens of millions of games compared to the tens of thousands played by a professional in his/her life). He however, noted a key difference that humans learn from other humans and textbooks, which in it of itself, contains thousands of years of human knowledge from playing the game. He joked that a fair comparison could be made by training an agent from scratch and comparing it against an agent encoded with the knowledge in Go textbooks, and evaluate exactly how many games of self-play that book was worth.

### Cogntively Informed AI

This was a really interesting workshop, and I'm bummed I missed the talks by Tom Griffiths and Yoshua Bengio on metagcognitive processes with rational models and learning higher level cognition from disentangled representations respectively.  I did catch Matt Botnivick's talk on meta-RL.  Essentially it proposes using an RL system (eg dopamine system) to train another system (eg prefrontal cortex), which will be more efficient and better suited for the task.
[1]http://www.ccnss.org/wp-content/uploads/2017/04/WangETAL_text_figures_methods_suppl.pdf

### Hierarchical Reinforcement Learning

Pieter Abbeel gave a talk on hierarchical RL.  He was enthusiastic about a hierarchical RL workshop since he has historically not had a lot of success in this area. Hierarchical RL is useful for solving problems like exploration, credit assignment, and transfer learning.  He cites Peter Dayan and Geoff Hinton's [Fedual RL paper](http://www.cs.toronto.edu/~fritz/absps/dh93.pdf) in 1993 as the most promising approach though there has not been much follow up since then.  Essentially it proposes a hierarchy of managers where the reward is passed downward onto submanagers.  Pieter discussed *option learning* and guessed that the eventual solution will consist of tens of layers.

### Pyro
![Pyro Workshop]({{ site.url }}/blog/images/pyro.png "Pyro NIPS Meetup")

Noah Goodman gave a workshop talk about our universal probabilistic programming language, [Pyro](http://pyro.ai).  Try it out!

## Papers

Below is a aggregation of papers I found interesting.  I want to first preface that the below is *heavily* subjected to selection bias, as there were thousands of posters, and I even missed a poster session because I became sick during the conference so I am fully aware that I may be omitting outstanding papers that deserve mention.

see pictures

REBAR

Program Induction

Inverse graphics

trust region variational inf

[Alpha Zero](https://arxiv.org/abs/1712.01815)



### Demos

![Libratus]({{ site.url }}/blog/images/libratus.png "Libratus")
Researchers at CMU demonstrated [Libratus](https://www.ijcai.org/proceedings/2017/0772.pdf), the heads-up no-limit Texas hold 'em bot that convincingly beat top professionals.  Played a few hands against it!

![Libratus](http://people.eecs.berkeley.edu/~cbfinn/_files/imitationsmall.gif "Robot")
*Image taken from https://sites.google.com/view/one-shot-imitation*
Pieter Abbeel's group demo-ed meta-imitation learning with a robot arm that was able to learn by demonstration and generalize to other objects.



## Events
### Uber @ NIPS
Uber hosted a party (joint between AI Labs and ATG) on the second night. It was completely full, and I met some really interesting folks!

![Uber Party]({{ site.url }}/blog/images/tesla.png "Uber Party")

![Pyro drink]({{ site.url }}/blog/images/pyro-margarita.png "Pyro Margarita :)")

### Tesla Fireside Chat
![Tesla]({{ site.url }}/blog/images/tesla.png "Elon Musk, Andrej Karpathy")

Tesla held a fireside chat with Elon Musk, Andrej Karpathy, and Jim Keller.  Some thoughts

* Tesla is developing its own hardware for AI computation, that will presumably run in its cars
* Elon does not think fusion on earth is needed. The sun provides more than enough energy, and we are currently using "a pitiful amount"
* Elon thinks AGI will happen in the next 5-7 years and will be a greater threat to loss of jobs than autonomous cars
* Jim does not believe that we will hit hard limitatinos on hardware. "I've been hearing those claims every 10 years for 30 years. I've decided to stop listening altogether"
* Andrej thinks progress towards AGI is a step function and will happen suddenly/unexpectedly.  If you look at the progress of AlphaGo Zero, there was a long period of engineering effort and algorithmic advances, then [reached superhuman status in 3 days](https://deepmind.com/blog/alphago-zero-learning-scratch/).

## Acknowledgements



## References
(see karpathys blog to see how he handled this?)
